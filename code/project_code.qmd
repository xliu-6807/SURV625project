---
title: SURV625 Applied Sampling
date: "`r format(Sys.time(), '%d %B, %Y')`"
embed-resources: true
editor: visual
execute:
  warning: false
  error: false
  tidy: true
format: pdf
---

```{r setup, include=FALSE, tidy=TRUE}
knitr::opts_chunk$set(echo=TRUE,cache=TRUE, 
                      autodep=TRUE, cache.comments=FALSE,
                      message=FALSE, warning=FALSE)

pacman::p_load(readxl, knitr, tidyverse)

options(scipen=999)
```

## __SM 625: Week 4 Sampling Project Notes__

### For each of the three variables that will be the focus of the final course project, the Department of Education would like to generate estimates of means and proportions having a coefficient of variation of no more than 0.05. Using the numbers provided to you in the description of the final project, compute estimates of the element variances for each variable. Given these estimates, compute the desired level of precision (the desired sampling variance) for each estimate that corresponds to the desired coefficient of variation.

### Now, given the desired levels of precision for each estimate, compute estimates of the necessary sample sizes for each of these three estimates (assuming simple random sampling), ignoring the finite population correction. These will be starting points for the eventual two-stage cluster sample design.


### We first build a table to store our results for each week's assignments.

-   We also add the expected averages for each outcome variable.

```{r}

# build dataframe with inputs
MI_school_samples <- tibble(
  Outcome = c("smoked_cig", "smoked_mj", "age_approached_to_smoke"),
  type = c("prop", "prop", "mean"),
  desire_cv = rep(.05, 3),
  expect_mean = c(.25, .15, 12)
) 
  # calculate element 
MI_school_samples |> kable()
```


\

#### Our process is to:

-   1st, calculate the estimated element variance.

    -   For a proportion, to get the element variance we use $\hat{p}(1-\hat{p})$.
    -   For a mean, to get the element variance we simply just square the estimated standard deviation $v(\bar{y}) = \sigma^2$.

-   2nd, we calculate the estimated standard error as $se(\hat{p}) = CV \times \hat{p}$.

-   3rd, we compute the desired sampling variance as: $var(\hat{p}) =  se(\hat{p})^2 \text{, where }  se(\hat{p}) = \sqrt{var(\hat{p})}$


```{r}
MI_school_samples <- MI_school_samples |> 
  mutate(
    # compute element variance
    s_sqrd = if_else(type=="prop", # for proportions
                               expect_mean * (1 - expect_mean), 
                               if_else(type=="mean",# for means
                                       1^2, NA)),
    # compute standard error
    se = desire_cv * expect_mean,
    # compute variance
    V = se^2     
    )

MI_school_samples |> select(-type) |> kable()

```




We now estimate the desired sample sizes when we desire a CV =.05 as $n = \frac{s^2}{se^2}$


```{r}
MI_school_samples <- MI_school_samples |> 
  mutate(SRS_n = s_sqrd / V)

MI_school_samples |> select(1, SRS_n) |> kable()
```




\newpage





## __SM 625: Week 5 Sampling Project Notes__

### For this week, we will consider the information available for stratified sampling of students. Eventually you are going to design a stratified cluster sample of students, where the clusters (or PSUs) are schools, but we aren’t there yet.

#### Recall the regions of interest in the sampling project description:

```{r}

school_frame <- read_xls(
  "~/repos/SURV625project/data/MI_school_frame_head_counts.xls")

```

```{r, echo=FALSE}
school_frame |> 
  select(Region, County_ID) |> 
  group_by(Region) |> 
  distinct(County_ID) |> 
  arrange(County_ID) |> 
  summarise_all(toString) |> 
  kable() # note, county 42 has 0 schools
  
```

### As “State officials are interested in providing, if at all possible, separate estimates for each of nine education regions in the state, where the regions are defined by groups of counties”, we will use these nine regions as strata.

### Prepare a table that includes the:

-   Overall population counts in each of these nine strata (the total count of students in the target population at each school is in the tot_all column on the sampling frame).

-   Given these counts, once you have the working overall sample size (unknown for now and will be decided by your team next week), what is the proportionate allocation plan of that sample of students across these nine strata?

```{r}

# we will use Region, County_ID, and tot_all

# region counts
school_frame |> 
  group_by(Region) |> 
  tally(tot_all) |> 
  mutate(prop_allocation = n/sum(n)) |> 
  rename(pop_count = n) |> 
  kable()


# what is the proportionate allocation plan of that sample 
## of students across these nine strata?


```





\newpage





## __SM 625: Week 6 Sampling Project Notes__

### From a previous study, you obtain estimates of the following design effects for each of these three estimates:

-   proportion ever smoked one cigarette = 2.5;

-   proportion ever smoked marijuana = 2.0; and

-   mean age when first asked to smoke = 1.7.\

### This previous study featured a sample of size n = 7,500 students between the ages of 13 and 19, selected from a total of a = 150 clusters. Using this information, compute a synthetic estimate of roh for each of the three variables. These synthetic estimates of roh will be used to consider alternative cluster sample designs as you continue with your project work. Finally, budget and cost information is now available. The total budget for data collection for this project will be \$500,000. The client and the data collection organization estimate that the data collection will cost \$3,000 per primary stage cluster (school), and \$50 per completed questionnaire within a cluster. We will use this cost information moving forward for optimal subsample size calculations.

\


We can estimate the sample ICC or roh from the given design effect estimate as:

$$
\hat{roh} = \frac{deff-1}{m-1}
$$

We now that the sample total is $nm=7500$ and the sample number of cluster is $n=150$, which we can take the mean cluster size as $m = nm/n = 7500/150 = 50$ and use it to calculate $roh$.

```{r}
nm <- 7500
n <- 150
m <- nm / n

MI_school_samples <- MI_school_samples |> 
  # add deff and roh to our table
  mutate(deff = c(2.5, 2.0, 1.7),
         # compute roh
         roh = (deff - 1) / (m - 1)
         )

MI_school_samples |> select(Outcome, deff, roh) |>  kable()
```



\newpage 




## SM 625: Week 7 Sampling Project Notes

Recall that the client and the data collection organization estimated that the data collection would cost \$3,000 per primary stage cluster (school), and \$50 per completed questionnaire within a cluster. We will now use this information for optimum subsample size calculations. Recall that the total budget for data collection will be \$500,000.

Given this cost information and your estimates of roh for the three different variables of primary interest from last week, compute the optimum subsample size (and the corresponding optimal number of first stage clusters, given the total budget above) for each of the variables. 

- How will you decide on a single overall optimum subsample size to use in your design? 

- Think about a comparison of alternative cluster sample designs: under a fixed cost constraint, how would we decide which design would be best? What will be your overall sample size (n) under this new optimum subsample size? 


As you make progress in writing up what you have done so far, provide some discussion of the rationale for your choices in this regard.

Next, given this optimum subsample size and treating the values of roh as portable, compute the new expected DEFF for each estimate given the new design (this can be specific to each variable / estimate, given the different optimum subsample sizes). In addition, compute a new expected SRS variance for each variable under the new design, using the new “optimum” overall sample size (remember that you can treat the element variances for each variable estimated last week as portable). Finally, compute the new expected sampling variance for each estimate under this new cluster sample design. Are you still meeting the client’s precision requirements?

The client has also provided other new information: the estimated size of the target population is N = 830,138. Given this population size and your overall sample size (n) under the new optimum subsample size computed above, what is your overall working sampling fraction (f)? Does it seem like finite population corrections will be necessary in your sampling variances if you choose to perform SRSWOR at some point?

The tables that you are developing and the text that accompanies them should carefully reflect the answers to all of the questions above.



\

We now have budget constraints and denote the cost per cluster as $c_n = \$3,000$ and cost per element as $c_m = \$50$, with a total budget constraint of $C = \$500,000$. Since we know there are $n = 150$ clusters and a total sample size of $7,500$ students.


To compute the optimum $m$ size we use the following equation:

$$
m_{opt} = \sqrt{\frac{c_n}{c_m} \frac{1-roh}{roh}}
$$

\

Finally, we compute the sampling cost as $n \times c_n + n \times m \times c_m$ which we defined these terms above.

```{r}
c_n = 3000 # cost per cluster
c_m = 50 # cost per element within cluster
C = 500000 # total budget

MI_school_samples <- MI_school_samples |> 
  mutate(
    # compute optimum m size
    m_opt = sqrt( (c_n / c_m) * ( (1-roh)/roh) ))

MI_school_samples |> select(Outcome, roh, m_opt) |> 
  kable()
```


We now use the optimum m size and our budget constraints to calculate the an optimal number of $n$ clusters for each outcome variable. For each outcome we use the optimum $m$ to find the optimum $n$ within our budget constraints as $\$500,000 = n(c_n + m \times c_m)$.

- We also compute the total sample size for these three options, and from here we can compute the total cost as $n \times  c_n + n \times m \times c_m$

```{r}
MI_school_samples <- MI_school_samples |> 
  mutate(
    # compute optimum n
    n_opt = C / (c_n + m_opt * c_m),
    # compute total SSU
    total_nm = m_opt * n_opt
  )

MI_school_samples |> 
  select(Outcome, roh, m_opt, n_opt, total_nm) |> 
  # we can print projected total cost for n=50
  mutate(projected_cost = (c_n * n_opt) + (c_m * n_opt * m_opt),
         projected_cost = scales::dollar(projected_cost)) |> 
  kable()
```

###### Note. if we round up for the optimum $m$ and $n$ we would slightly be above the total budget. 

To be continued . . . . Answer . . . 

How will you decide on a single overall optimum subsample size to use in your design?

- We will need to use the optimim n and m of each row and project them to the other rows to calculate the design effect: deff = (SAM_variance / SRS_variance). This should give us an idea of the mean deff for three total projections. 





